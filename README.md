# ğŸ¤– Comprehensive Artificial Intelligence Reading Compendium

<div align="center">

![AI Compendium](https://img.shields.io/badge/AI-Compendium-blue?style=for-the-badge&logo=artificial-intelligence)
[![Contributors Welcome](https://img.shields.io/badge/Contributors-Welcome-brightgreen?style=for-the-badge)](#-contributing)
[![GitHub Stars](https://img.shields.io/github/stars/gr8monk3ys/ai-compendium?style=for-the-badge&logo=github)](https://github.com/gr8monk3ys/ai-compendium/stargazers)
[![Last Updated](https://img.shields.io/badge/Updated-January%202025-orange?style=for-the-badge)](#)

*ğŸ¯ A curated collection of **free**, high-quality resources for learning artificial intelligence*

</div>

---

## ğŸŒŸ About This Compendium

A comprehensive, regularly updated collection of AI and machine learning resourcesâ€”from foundational textbooks to cutting-edge 2024-2025 research papers. Whether you're just starting out or pushing the boundaries of AI research, you'll find valuable resources here.

### ğŸ¯ Who Is This For?

| ğŸ‘¨â€ğŸ“ **Students** | ğŸ‘©â€ğŸ’» **Developers** | ğŸ”¬ **Researchers** | ğŸ¢ **Professionals** |
|---|---|---|---|
| Building foundational knowledge | Implementing AI solutions | Staying current with research | Understanding AI for business |

---

## ğŸš€ Quick Start

### For Beginners ğŸ‘¶
1. Start with [General AI & ML Foundations](#general-ai--machine-learning-foundations)
2. Move to [Neural Networks & Deep Learning](#neural-networks--deep-learning) basics
3. Explore [Ethics & Safety](#ethics-safety--ai-alignment) considerations

### For Intermediate Learners ğŸ¯  
1. Dive into [Transformers & LLMs](#transformers--large-language-models-llms)
2. Explore [Generative AI](#generative-ai--generative-models) techniques
3. Study [Multi-Modal AI](#multi-modal-ai-vision-language-and-more) applications

### For Advanced Practitioners ğŸš€
1. Focus on cutting-edge research papers in each section
2. Explore [Neuro-Symbolic AI](#neuro-symbolic--hybrid-ai) developments
3. Study [Mechanistic Interpretability](#mechanistic-interpretability-new) research
4. Contribute to the community by suggesting new resources

---

## ğŸ“– Table of Contents

1. [General AI & Machine Learning Foundations](#general-ai--machine-learning-foundations)
2. [Neural Networks & Deep Learning](#neural-networks--deep-learning)
3. [Transformers & Large Language Models (LLMs)](#transformers--large-language-models-llms)
4. [Generative AI & Generative Models](#generative-ai--generative-models)
5. [Reinforcement Learning & Decision Making](#reinforcement-learning--decision-making)
6. [Symbolic AI & Automated Reasoning](#symbolic-ai--automated-reasoning)
7. [Cognitive Architectures & Cognitive Modeling](#cognitive-architectures--cognitive-modeling)
8. [Neuro-Symbolic & Hybrid AI](#neuro-symbolic--hybrid-ai)
9. [Multi-Modal AI (Vision, Language, and More)](#multi-modal-ai-vision-language-and-more)
10. [Explainability & Model Interpretability](#explainability--model-interpretability)
11. [Ethics, Safety & AI Alignment](#ethics-safety--ai-alignment)
12. [Human-AI Interaction & Collaboration](#human-ai-interaction--collaboration)

---

## General AI & Machine Learning Foundations
*Introductory and broad resources on AI and machine learning, suitable for building a strong foundation.*

### ğŸ“š Textbooks & Books

* **[Artificial Intelligence: Foundations of Computational Agents (3rd Ed., 2023)](https://artint.info/)** â€“ *David L. Poole & Alan K. Mackworth*. Comprehensive HTML textbook covering intelligent agents, search, reasoning under uncertainty, planning, multi-agent systems, and societal impacts. **Major 2023 revision with deep learning and AI ethics coverage.**
* **[Deep Learning: Foundations and Concepts (2024)](https://www.bishopbook.com/)** â€“ *Christopher Bishop & Hugh Bishop*. ğŸ†• **#1 AI bestseller 2024-2025**. First major textbook covering transformers and diffusion models with rigorous mathematical foundations. Endorsed by LeCun and Bengio.
* **[Understanding Deep Learning (2023)](https://udlbook.github.io/udlbook/)** â€“ *Simon J.D. Prince*. MIT Press textbook with free online version. Comprehensive coverage of modern architectures including transformers, diffusion models. Described as "instant classic" by Nature Machine Intelligence.
* **[The Hundred-Page Machine Learning Book (2019)](http://themlbook.com/wiki/doku.php)** â€“ *Andriy Burkov*. 100-page ML primer, free PDF available under "read first, pay later."
* **[Machine Learning Yearning (2018)](https://www.mlyearning.org/)** â€“ *Andrew Ng*. Free online guide on ML project strategy: task prioritization, error analysis, validation.
* **[Introduction to Statistical Learning (2nd Ed., 2021)](https://www.statlearning.com/)** â€“ *Gareth James et al.* Free PDF textbook with R labs covering regression, classification, trees, clustering. **Python Edition (ISLP) also available (2023).**
* **[Elements of Statistical Learning (2009)](https://hastie.su.domains/ElemStatLearn/)** â€“ *Trevor Hastie et al.* In-depth theory book on SVMs, boosting, neural nets, available free.
* **[Probabilistic Machine Learning: Advanced Topics (2023)](https://probml.github.io/pml-book/book2.html)** â€“ *Kevin Murphy*. ğŸ†• Authoritative advanced ML reference with modern probabilistic methods.

### ğŸ“„ Introductory Resources

* **[Demystifying Artificial Intelligence (2023)](https://freecomputerbooks.com/Demystifying-Artificial-Intelligence.html)** â€“ *Emmanuel Gillain*. Concise open-access book contrasting symbolic/statistical AI and explaining core concepts and ethics.
* **[Unlocking Artificial Intelligence (2022)](https://freecomputerbooks.com/Unlocking-Artificial-Intelligence.html)** â€“ *Christopher Mutschler et al.* Springer PDF on data-driven learning, uncertainty quantification, RL agents, and industrial applications.

---

## Neural Networks & Deep Learning
*Resources on neural architectures and deep learning, from intuitive introductions to comprehensive references.*

### ğŸ“š Foundational Texts

* **[Neural Networks and Deep Learning (2015)](http://neuralnetworksanddeeplearning.com/)** â€“ *Michael Nielsen*. Free online book with narrative explanation, visual proofs, and code walkthroughs. *Note: Uses Python 2.6/Theanoâ€”concepts remain excellent.*
* **[Deep Learning (2016)](https://www.deeplearningbook.org/)** â€“ *Goodfellow, Bengio & Courville*. Standard deep learning textbook, free HTML/PDF. Foundational but predates transformers.
* **[Dive into Deep Learning (Cambridge University Press, 2023)](https://d2l.ai/)** â€“ *Zhang et al.* Jupyter-based interactive book with PyTorch, TensorFlow, JAX examples. **Adopted by 500+ universities worldwide.**
* **[A Brief Introduction to Neural Networks (2014)](https://www.dkriesel.com/en/science/neural_networks)** â€“ *David Kriesel*. PDF with hand-drawn illustrations explaining perceptrons, backprop.

### ğŸ“ Practical Courses

* **[Deep Learning for Coders with fastai and PyTorch (2020)](https://course.fast.ai/)** â€“ *Howard & Gugger*. Fast.ai's free online book teaching practical DL via high-level API. **New "How to Solve It With Code" course released November 2024.**
* **[Deep Learning with PyTorch (2020)](https://github.com/PacktPublishing/Deep-Learning-with-PyTorch)** â€“ *Stevens, Antiga & Viehmann*. GitHub repo for free download of PyTorch tutorial book.

### ğŸ“– Historical & Cultural Context

* **[Neural Networks (2023)](https://library.oapen.org/handle/20.500.12657/69382)** â€“ *Dhaliwal, Lepage-Richer & Suchman*. OAPEN book exploring neural nets' history, culture, and societal impact.

---

## Transformers & Large Language Models (LLMs)
*Key papers and surveys on attention, transformer architectures, and modern LLM training techniques.*

### ğŸ›ï¸ Foundational Papers (Historical)

* **[Attention Is All You Need (2017)](https://arxiv.org/abs/1706.03762)** â€“ *Vaswani et al.* Introduced the Transformer, revolutionizing sequence modeling with self-attention.
* **[BERT: Pre-training of Deep Bidirectional Transformers (2018)](https://arxiv.org/abs/1810.04805)** â€“ *Devlin et al.* Bidirectional masked LM and next-sentence prediction.
* **[Language Models are Few-Shot Learners (2020)](https://arxiv.org/abs/2005.14165)** â€“ *Brown et al.* GPT-3 paper showing few-shot prompting capabilities.
* **[Training Language Models to Follow Instructions with Human Feedback (2022)](https://arxiv.org/abs/2203.02155)** â€“ *Ouyang et al.* InstructGPT/RLHF method to align LMs to user intent.

### ğŸš€ Frontier Model Technical Reports (2024-2025) ğŸ†•

| Model | Organization | Paper | Key Innovations |
|-------|-------------|-------|-----------------|
| **GPT-4o** | OpenAI | [System Card (Oct 2024)](https://arxiv.org/abs/2410.21276) | Omni-modal (text, audio, image, video), 232ms audio response |
| **Claude 3.5 Sonnet** | Anthropic | [Model Card (2024)](https://www-cdn.anthropic.com/fed9cc193a14b84131812372d8d5857f8f304c52/Model_Card_Claude_3_Addendum.pdf) | 64% on agentic coding, computer use capability |
| **Gemini 1.5/2.5** | Google DeepMind | [Technical Report (2024)](https://arxiv.org/abs/2403.05530) | 1M-10M token context, Sparse MoE, native tool use |
| **Llama 3/3.1** | Meta | [Technical Report (Jul 2024)](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/) | 405B parameters, 128K context, open weights |
| **DeepSeek-V3** | DeepSeek | [Technical Report (Dec 2024)](https://arxiv.org/abs/2412.19437) | 671B MoE, trained for ~$5.5M, Multi-head Latent Attention |
| **DeepSeek-R1** | DeepSeek | [Technical Report (Jan 2025)](https://github.com/deepseek-ai/DeepSeek-R1) | GRPO reasoning, competitive with o1, open weights |
| **Qwen2.5** | Alibaba | [Technical Report (Dec 2024)](https://arxiv.org/abs/2412.15115) | 18T tokens, 128K context, 29+ languages |
| **Phi-4** | Microsoft | [Technical Report (Dec 2024)](https://www.microsoft.com/en-us/research/publication/phi-4-technical-report/) | 14B parameters, surpasses GPT-4 on STEM via data quality |
| **Mixtral 8x22B** | Mistral | [Blog (Apr 2024)](https://mistral.ai/news/mixtral-8x22b) | 141B total/39B active MoE, Apache 2.0 license |

### ğŸ“Š Comprehensive Surveys

* **[A Survey of Large Language Models (2023, continuously updated)](https://arxiv.org/abs/2303.18223)** â€“ *Zhao et al.* RUCAIBox 400+ page comprehensive survey. [GitHub](https://github.com/RUCAIBox/LLMSurvey)
* **[Foundations of Large Language Models (2023)](https://arxiv.org/abs/2307.09394)** â€“ *Tong Xiao & Jingbo Zhu*. Comprehensive survey of LLM pre-training, scaling, fine-tuning.
* **[Efficient Large Language Models: A Survey (2024)](https://arxiv.org/abs/2312.03863)** â€“ *Wan et al.* ğŸ†• TMLR systematic review of quantization, pruning, knowledge distillation.
* **[A Survey on Mixture of Experts in LLMs (2024)](https://arxiv.org/abs/2407.06204)** â€“ ğŸ†• TKDE comprehensive MoE architecture review.

### ğŸ§  Advanced LLM Techniques ğŸ†•

#### Chain-of-Thought & Reasoning
* **[Towards Reasoning Era: A Survey of Long Chain-of-Thought (2025)](https://arxiv.org/abs/2503.09567)** â€“ Long CoT vs Short CoT analysis
* **[Latent Chain-of-Thought Reasoning Survey (2025)](https://arxiv.org/abs/2505.16782)** â€“ Reasoning in latent spaces

#### AI Agents
* **[A Survey on LLM-based Autonomous Agents (2023, updated 2025)](https://arxiv.org/abs/2308.11432)** â€“ Unified framework for LLM agents
* **[LLM Powered Autonomous Agents (2023)](https://lilianweng.github.io/posts/2023-06-23-agent/)** â€“ *Lilian Weng*. Influential tutorial on agent architectures

#### RAG (Retrieval-Augmented Generation)
* **[RAG for LLMs: A Survey (2023)](https://arxiv.org/abs/2312.10997)** â€“ Naive/Advanced/Modular RAG paradigms
* **[Agentic RAG Survey (2025)](https://arxiv.org/abs/2501.09136)** â€“ ğŸ†• Autonomous agents in RAG pipelines

### ğŸ“œ Historical References

* **[GPT-4 Technical Report (2023)](https://cdn.openai.com/papers/gpt-4.pdf)** â€“ *OpenAI*. Official GPT-4 capabilities and limitations.
* **[Sparks of AGI: Early Experiments with GPT-4 (2023)](https://arxiv.org/abs/2303.12712)** â€“ *Bubeck et al.* Empirical evaluation of GPT-4's general intelligence.
* **[LLaMA: Open and Efficient Foundation Language Models (2023)](https://arxiv.org/abs/2302.13971)** â€“ *Touvron et al.* Meta's original LLaMA models.
* **[BLOOM Language Model (2022)](https://arxiv.org/abs/2211.05100)** â€“ *BigScience*. 176B open-source multilingual LLM.

---

## Generative AI & Generative Models
*Seminal papers on GANs, VAEs, diffusion models, and state-of-the-art generative techniques.*

### ğŸ›ï¸ Foundational Papers

* **[Generative Adversarial Networks (2014)](https://arxiv.org/abs/1406.2661)** â€“ *Goodfellow et al.* Original GAN formulation as minimax game.
* **[Auto-Encoding Variational Bayes (2013)](https://arxiv.org/abs/1312.6114)** â€“ *Kingma & Welling*. Original VAE paper.
* **[Denoising Diffusion Probabilistic Models (2020)](https://arxiv.org/abs/2006.11239)** â€“ *Ho et al.* Diffusion-based generative modeling.
* **[Latent Diffusion Models (2022)](https://arxiv.org/abs/2112.10752)** â€“ *Rombach et al.* Stable Diffusion's latent-space approach.
* **[CycleGAN: Unpaired Image-to-Image Translation (2017)](https://arxiv.org/abs/1703.10593)** â€“ *Zhu et al.* GANs for style transfer without paired data.

### ğŸ–¼ï¸ Image Generation (2024-2025) ğŸ†•

| Model | Organization | Paper/Link | Key Innovation |
|-------|-------------|------------|----------------|
| **Stable Diffusion 3** | Stability AI | [Paper (Mar 2024)](https://stability.ai/news/stable-diffusion-3-research-paper) | MMDiT architecture, Rectified Flow (abandons U-Net) |
| **FLUX.1/2** | Black Forest Labs | [GitHub (2024)](https://github.com/black-forest-labs/flux) | State-of-the-art open-weight, latent flow matching |
| **DALL-E 3** | OpenAI | [Paper (Sep 2023)](https://cdn.openai.com/papers/dall-e-3.pdf) | Synthetic recaptioning for prompt following |
| **Imagen 3** | Google DeepMind | [Blog (May 2024)](https://deepmind.google/technologies/imagen-3/) | Highest-quality text-to-image from Google |

### ğŸ¬ Video Generation ğŸ†•

| Model | Organization | Link | Key Innovation |
|-------|-------------|------|----------------|
| **Sora** | OpenAI | [Technical Report (Feb 2024)](https://openai.com/index/video-generation-models-as-world-simulators/) | DiT on spacetime patches, "world simulation" framing |
| **Veo/Veo 2/Veo 3** | Google DeepMind | [Blog](https://deepmind.google/technologies/veo/) | 4K output, Veo 3 generates synchronized audio |
| **Runway Gen-3/4** | Runway | [Website](https://runwayml.com/) | Director Mode, identity consistency |
| **Lumiere** | Google | [Paper (Jan 2024)](https://arxiv.org/abs/2401.12945) | Space-Time U-Net, single-pass full video |

### ğŸµ Audio Generation ğŸ†•

| Platform | Description |
|----------|-------------|
| **[Suno AI](https://suno.ai/)** | Full-song generation (lyrics + vocals + instrumentation), v5 extends to 8 minutes |
| **[Udio](https://udio.com/)** | High-fidelity vocals, founded by ex-DeepMind researchers |
| **[Stable Audio 2.0](https://stability.ai/stable-audio)** | Instrumental-focused, claims licensed training data |
| **[ElevenLabs](https://elevenlabs.io/)** | State-of-the-art voice cloning, 32+ languages |

* **[AI Music Generation Survey (2025)](https://www.mdpi.com/2078-2489/16/8/656)** â€“ ğŸ†• Comprehensive review of music generation techniques

### ğŸ® 3D Generation ğŸ†•

* **[3D Gaussian Splatting (SIGGRAPH 2023)](https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/)** â€“ *Kerbl et al.* Game-changer for real-time 3D rendering (â‰¥30fps at 1080p). Replaces NeRF for many applications.
* **[TRELLIS (CVPR 2025 Spotlight)](https://github.com/microsoft/TRELLIS)** â€“ ğŸ†• *Microsoft*. Up to 2B parameters, outputs Radiance Fields, 3D Gaussians, and meshes.

### ğŸ“„ Other Foundational Resources

* **[Imagen & DALLÂ·E 2 (2022)](https://cdn.openai.com/papers/dall_e_2.pdf)** â€“ Reports on advanced text-to-image methods.
* **[Prompting Techniques Survey (2021)](https://arxiv.org/abs/2107.13586)** â€“ *Liu et al.* Systematic review of prompting methods in NLP.
* **[OpenAI Jukebox (2020)](https://arxiv.org/abs/2005.00341)** â€“ Music generation with transformers.
* **[AudioLM (2022)](https://arxiv.org/abs/2201.05424)** â€“ Token-based audio generation.

---

## Reinforcement Learning & Decision Making
*Core RL textbooks, landmark papers, and modern advancements.*

### ğŸ“š Core Textbooks

* **[Reinforcement Learning: An Introduction (2nd Ed., 2018)](http://incompleteideas.net/book/the-book.html)** â€“ *Sutton & Barto*. Definitive RL textbook, free PDF. Still the gold standard.
* **[Algorithms for Decision-Making (2022)](https://algorithmsbook.com/)** â€“ *Kochenderfer et al.* Decision theory, planning, bandits, and RL.
* **[Reinforcement Learning: An Overview (Dec 2024)](https://arxiv.org/abs/2412.05265)** â€“ *Kevin Murphy*. ğŸ†• Comprehensive modern survey covering LLMs+RL.

### ğŸ›ï¸ Landmark Papers

* **[AlphaZero (2017)](https://arxiv.org/abs/1712.01815)** â€“ *Silver et al.* Self-play RL with MCTS for Chess/Shogi/Go.
* **[Planning Algorithms (2006)](http://planning.cs.uiuc.edu/)** â€“ *LaValle*. Motion and discrete planning.

### ğŸ› ï¸ Environments & Tools

* **[Gymnasium (2024)](https://gymnasium.farama.org/)** â€“ ğŸ†• **Replaces deprecated OpenAI Gym**. Maintained by Farama Foundation. [Paper](https://arxiv.org/abs/2407.17032)
  * âš ï¸ **Important**: OpenAI Gym is no longer maintained. Use Gymnasium instead.
  * New API: `step()` returns 5 values (observation, reward, terminated, truncated, info)

### ğŸ¤– RL for LLM Training ğŸ†•

| Method | Paper | Key Innovation |
|--------|-------|----------------|
| **DPO** | [NeurIPS 2023](https://arxiv.org/abs/2305.18290) | Eliminates reward model, 40% faster than RLHF |
| **Constitutional AI** | [Anthropic 2022](https://arxiv.org/abs/2212.08073) | RLAIF, self-critique/revision |
| **KTO** | [2024](https://arxiv.org/abs/2402.01306) | Kahneman-Tversky Optimization, works with binary labels |
| **GRPO** | DeepSeek-R1 (Jan 2025) | Group Relative Policy Optimization, no critic network |

* **[Comprehensive DPO Survey (Oct 2024)](https://arxiv.org/abs/2410.15595)** â€“ ğŸ†• Survey of 20+ DPO variants and applications

---

## Symbolic AI & Automated Reasoning
*Logic-based AI, knowledge representation, classical search, and symbolic planning.*

### ğŸ“š Foundational Resources

* **[Symbolic AI Overview: A Beginner's Guide (2019)](https://towardsdatascience.com/a-beginners-guide-to-symbolic-reasoning-and-deep-learning-1dea1f0d9db8)** â€“ *Artirm Gubaidullin*. Contrasts rule-based symbolic systems with data-driven deep learning.
* **[Knowledge Representation and Reasoning â€“ Stanford CS227 Notes](https://web.stanford.edu/class/cs227/)** â€“ *Brachman & Levesque*. Covers logic, semantic networks, frames, description logics.
* **[Learn Prolog Now!](https://lpn.swi-prolog.org/)** â€“ Free interactive Prolog tutorial teaching facts, rules, and queries.
* **[A* Search Algorithm (1968)](https://en.wikipedia.org/wiki/A*_search_algorithm)** â€“ *Hart, Nilsson & Raphael*. Wikipedia summary with pseudocode and complexity analysis.
* **[Alphaâ€“Beta Pruning (1975)](https://en.wikipedia.org/wiki/Alpha%E2%80%93beta_pruning)** â€“ *Knuth & Moore*. Minimax search optimization for game-playing AI.
* **[STRIPS Planner (1971)](https://ai.stanford.edu/~nilsson/OnlinePubs-Nils/PublishedPapers/strips.pdf)** â€“ *Fikes & Nilsson*. Foundation of classical planning.
* **[OWL 2 Web Ontology Language Overview (2009)](https://www.w3.org/TR/owl2-overview/)** â€“ W3C specification for knowledge graphs and ontologies.

### ğŸ†• Knowledge Graphs + LLMs

* **[Unifying LLMs and Knowledge Graphs: A Roadmap (2024)](https://arxiv.org/abs/2306.08302)** â€“ *Pan et al.* IEEE TKDE survey on KG-LLM integration
* **[GraphRAG (Microsoft Research, 2024)](https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/)** â€“ ğŸ†• LLM-generated knowledge graphs for enhanced RAG

### ğŸ†• Neural Theorem Proving

* **[LEGO-Prover (ICLR 2024 Oral)](https://openreview.net/forum?id=3f5PALef5B)** â€“ Growing skill library, SOTA on miniF2F
* **[Lean Copilot (2024)](https://arxiv.org/abs/2404.12534)** â€“ ğŸ†• LLMs native in Lean, automates 74.2% of proof steps
* **[Deep Learning for Theorem Proving Survey](https://github.com/zhaoyu-li/DL4TP)** â€“ Comprehensive resource list

---

## Cognitive Architectures & Cognitive Modeling
*Integrated architectures modeling human cognition: memory, reasoning, learning, and consciousness.*

### ğŸ›ï¸ Classic Architectures

* **[Soar Cognitive Architecture](https://soar.eecs.umich.edu/)** â€“ *John Laird*. Production-rule memory, working-memory decision cycle, chunking learning. **Active development with 2024 modernization.**
* **[ACT-R](http://act-r.psy.cmu.edu/)** â€“ *John R. Anderson*. Official website with free manual and papers on declarative/procedural memory modules. ACT-R 7 available.
* **[40 Years of Cognitive Architectures (2020)](https://arxiv.org/abs/2008.02700)** â€“ *Kotseruba & Tsotsos*. Survey of 84 architectures (symbolic, connectionist, hybrid).

### ğŸ¤– LLM-Based Cognitive Architectures ğŸ†•

* **[Cognitive Architectures for Language Agents (CoALA, 2024)](https://arxiv.org/abs/2309.02427)** â€“ *Sumers et al.* TMLR paper integrating LLMs with production-system memories and decision procedures.
* **[LLM-based Multi-Agent Systems Survey (IJCAI 2024)](https://arxiv.org/abs/2402.01680)** â€“ ğŸ†• Framework/orchestration/problem-solving categories
* **[Landscape of AI Agent Architectures (2024)](https://arxiv.org/abs/2404.11584)** â€“ ğŸ†• Taxonomy of modern agent designs

### ğŸ§  Memory Architectures for Agents ğŸ†•

* **[MemGPT](https://memgpt.ai/)** â€“ Hierarchical memory (primary context + external storage), OS paradigm
* **[Mem0 Research](https://mem0.ai/research)** â€“ 26% accuracy boost, 91% lower latency for agent memory
* **[CogMem (Dec 2024)](https://arxiv.org/abs/2512.14118)** â€“ ğŸ†• 3-layer cognitive science-inspired memory architecture
* **[HippoRAG](https://arxiv.org/abs/2405.14831)** â€“ Hippocampus-inspired memory consolidation

### ğŸ“– Consciousness & Theory

* **[LEIA: Language-Endowed Intelligent Agents (2023)](https://direct.mit.edu/books/monograph/5642/LEIALanguage-Endowed-Intelligent-Agents)** â€“ *McShane et al.* MIT Press open-access on hybrid symbolic + data-driven agents.
* **[Human and Machine Consciousness (2017)](https://www.openbookpublishers.com/books/10.11647/OBP.0107)** â€“ *David Gamez*. Free text exploring theories of consciousness in biological and artificial systems.

---

## Neuro-Symbolic & Hybrid AI
*Bridging neural networks with symbolic reasoning for robust, explainable systems.*

### ğŸ“Š Surveys & Compendia

* **[Neuro-Symbolic AI: A Survey (2023)](https://dl.acm.org/doi/10.1145/3592378)** â€“ *Garcez et al.* CACM survey of neural-symbolic frameworks (Neural Theorem Provers, DeepProbLog, Logic Tensor Networks).
* **[Compendium of Neurosymbolic Artificial Intelligence (2023)](https://ebooks.iospress.nl/volume/compendium-of-neurosymbolic-artificial-intelligence)** â€“ IOS Press, 30 foundational papers
* **[Neuro-Symbolic AI in 2024: A Systematic Review (Jan 2025)](https://arxiv.org/abs/2501.05435)** â€“ ğŸ†• *Colelough & Regli*. PRISMA review of 167 papers
* **[Towards Cognitive AI Systems: A Neuro-Symbolic Survey (2024)](https://arxiv.org/abs/2402.05123)** â€“ *Hao et al.* Taxonomy of integration strategies.

### ğŸ”¬ Key Systems & Papers

* **[Logical Neural Networks (2020)](https://arxiv.org/abs/2009.02506)** â€“ *Riegel et al.* Differentiable logic operators enabling exact Boolean reasoning. [GitHub](https://github.com/IBM/LNN)
* **[Neuro-Symbolic Concept Learner (2019)](https://arxiv.org/abs/1904.07250)** â€“ *Mao et al.* NS-CL for CLEVR visual question answering. [Website](http://nscl.csail.mit.edu/)
* **[Neural LP: Learning to Reason with Logic Programming (2017)](https://arxiv.org/abs/1707.06690)** â€“ *Yang et al.* Neural methods for learning logical rules over knowledge graphs.

### ğŸ†• LLM + Symbolic Reasoning

* **[Faithful Logical Reasoning via SymbCoT (2024)](https://arxiv.org/abs/2405.18357)** â€“ Translator/Planner/Solver/Verifier modules
* **[Large Language Models Are Neurosymbolic Reasoners (2024)](https://arxiv.org/abs/2401.09334)** â€“ 88% accuracy on symbolic tasks
* **[Can KGs Reduce Hallucinations in LLMs? (NAACL 2024)](https://aclanthology.org/2024.naacl-long.219/)** â€“ ğŸ†• Knowledge graphs for grounding

### ğŸ“š Additional Resources

* **[IBM Neurologic AI Workshop (2020)](https://www.ibm.com/blogs/research/2020/02/neurologic-ai-workshop/)** â€“ Slides and videos on prototype neuro-symbolic systems.
* **[KG-LLM Papers Collection](https://github.com/zjukg/KG-LLM-Papers)** â€“ Curated list of knowledge graph + LLM research

---

## Multi-Modal AI (Vision, Language, and More)
*Models that jointly process text, images, audio, and other modalities.*

### ğŸ›ï¸ Foundational Vision-Language Models

* **[CLIP (2021)](https://arxiv.org/abs/2103.00020)** â€“ *Radford et al.* Contrastive pre-training on 400M imageâ€“text pairs enabling zero-shot vision tasks.
* **[ALIGN (2021)](https://arxiv.org/abs/2102.05918)** â€“ *Jia et al.* Google's large-scale imageâ€“text contrastive model on 1B+ pairs.
* **[BLIP: Bootstrapping Language-Image Pre-training (2022)](https://arxiv.org/abs/2201.12086)** â€“ *Li et al.* Unified framework for visionâ€“language tasks.
* **[FLAVA (2022)](https://arxiv.org/abs/2203.14239)** â€“ *Singh et al.* Facebook's foundational model covering vision, language, and their fusion.

### ğŸš€ Multimodal LLMs (2024-2025) ğŸ†•

| Model | Organization | Paper | Key Capability |
|-------|-------------|-------|----------------|
| **GPT-4V/GPT-4o** | OpenAI | [System Card (Oct 2024)](https://arxiv.org/abs/2410.21276) | Native omni-modal |
| **Gemini 1.5/2.5** | Google | [Paper (2024)](https://arxiv.org/abs/2403.05530) | 1M-10M token context |
| **Claude 3.5 Vision** | Anthropic | [Model Card](https://www.anthropic.com/claude) | Document/chart analysis |
| **LLaVA-NeXT** | UW/Microsoft | [Blog (Jan 2024)](https://llava-vl.github.io/blog/2024-01-30-llava-next/) | 4x resolution improvement |
| **LLaVA-OneVision** | Multiple | [Paper (Aug 2024)](https://arxiv.org/abs/2408.03326) | First single model for image/multi-image/video |
| **Qwen2-VL/2.5-VL** | Alibaba | [Paper (Sep 2024)](https://arxiv.org/abs/2409.12191) | Dynamic resolution, matches GPT-4o |
| **InternVL 2.5/3** | OpenGVLab | [Paper (Dec 2024)](https://arxiv.org/abs/2412.05271) | First open-source >70% on MMMU |
| **SigLIP 2** | Google | [Paper (Feb 2025)](https://arxiv.org/abs/2502.14786) | ğŸ†• SOTA encoder with NaFlex (native aspect ratio) |

### ğŸ¬ Video Understanding ğŸ†•

* **[Video-LLaVA (EMNLP 2024)](https://arxiv.org/abs/2311.10122)** â€“ Unified visual representation for images and videos
* **[MovieChat (CVPR 2024)](https://github.com/rese1f/MovieChat)** â€“ Long video understanding with sparse memory
* **[LongVU (Oct 2024)](https://arxiv.org/abs/2410.17434)** â€“ ğŸ†• Extended context for video comprehension

### ğŸ”Š Audio-Visual Models ğŸ†•

* **[Meta Perception Encoder (PE-AV)](https://ai.meta.com/research/publications/audiovisual-pe/)** â€“ ğŸ†• Joint audio-video-text embedding
* **[Voxtral (Mistral, 2025)](https://mistral.ai/)** â€“ ğŸ†• First audio-native LLM family
* **[Voice-LLM Trends 2025](https://www.turing.com/resources/voice-llm-trends)** â€“ Overview of speech + LLM integration

### ğŸ“Š Surveys

* **[Multimodal Foundation Models: From Specialists to General-Purpose Assistants (2023)](https://arxiv.org/abs/2307.08007)** â€“ *Li et al.* Evolution from single-task to unified multimodal LLMs.
* **[A Survey on Multimodal LLMs (2024)](https://arxiv.org/abs/2306.13549)** â€“ *Yin et al.* IEEE TPAMI comprehensive overview.

### ğŸ“š Practical Resources

* **[Programming Computer Vision with Python (2012)](https://programmingcomputervision.com/)** â€“ *Jan Erik Solem*. Free book on image processing with PIL and OpenCV.
* **[CVPR 2024 Tutorials](https://cvpr.thecvf.com/Conferences/2024)** â€“ Latest vision foundation model tutorials
* **[NeurIPS 2024 Tutorials](https://neurips.cc/)** â€“ Multimodal and foundational model tutorials

---

## Explainability & Model Interpretability
*Techniques and critiques for making AI models transparent and understandable.*

### ğŸ“š Foundational Resources

* **[Interpretable Machine Learning (2019, continuously updated)](https://christophm.github.io/interpretable-ml-book/)** â€“ *Christoph Molnar*. Free guide covering feature importance, LIME, SHAP, saliency maps, and best practices.
* **[The Building Blocks of Interpretability (2018)](https://distill.pub/2018/building-blocks/)** â€“ *Olah et al.* Interactive Distill.pub article visualizing internal neuron activations in CNNs.
* **[Attention Is Not Explanation (2019)](https://arxiv.org/abs/1902.10186)** â€“ *Jain & Wallace*. Shows attention weights may not correspond to model reasoning.

### ğŸ”§ Explanation Methods

* **[SHAP: SHapley Additive exPlanations (2017)](https://arxiv.org/abs/1705.07874)** â€“ *Lundberg & Lee*. Game-theoretic framework for local feature attributions. Now supports HuggingFace Transformers.
* **[LIME (2016)](https://github.com/marcotcr/lime)** â€“ Local Interpretable Model-agnostic Explanations
* **[TCAV: Testing with Concept Activation Vectors (2018)](https://arxiv.org/abs/1711.11279)** â€“ *Kim et al.* Probes whether high-level human concepts influence predictions.

### ğŸ”¬ Mechanistic Interpretability ğŸ†•

**Named MIT Technology Review "Breakthrough Technology 2026"**

#### Anthropic Research ([transformer-circuits.pub](https://transformer-circuits.pub/))
* **[Scaling Monosemanticity (May 2024)](https://transformer-circuits.pub/2024/scaling-monosemanticity/)** â€“ 34M feature SAE on Claude Sonnet
* **[Circuit Tracing (March 2025)](https://transformer-circuits.pub/2025/attribution-graphs/)** â€“ ğŸ†• Attribution graphs for tracing computation
* **[On the Biology of a Large Language Model (March 2025)](https://transformer-circuits.pub/2025/biology/)** â€“ ğŸ†• Applied to Claude 3.5 Haiku
* **[Open-source Circuit Tracer](https://github.com/safety-research/circuit-tracer)** â€“ ğŸ†• Tools for mechanistic interpretability

#### Google DeepMind Gemma Scope
* **[Gemma Scope (July 2024)](https://deepmind.google/blog/gemma-scope-helping-the-safety-community-shed-light-on-the-inner-workings-of-language-models/)** â€“ 400+ SAEs, 30M+ features for Gemma 2
* **[Gemma Scope 2 (December 2025)](https://deepmind.google/discover/blog/)** â€“ ğŸ†• ~110 PB activation data, transcoders on every layer

#### Tools for Mechanistic Interpretability

| Tool | URL | Description |
|------|-----|-------------|
| **TransformerLens** | [GitHub](https://github.com/TransformerLensOrg/TransformerLens) | Mechanistic interpretability for GPT-style models |
| **SAE Lens** | [PyPI](https://pypi.org/project/sae-lens/) | Training and analyzing Sparse Autoencoders |
| **Neuronpedia** | [Website](https://www.neuronpedia.org/) | 50M+ searchable features, circuit visualization |

### ğŸ“‹ Documentation Standards

* **[Model Cards (2019)](https://arxiv.org/abs/1810.03993)** â€“ *Mitchell et al.* Documentation of model performance, intended use, and limitations. Now industry standard.
* **[Datasheets for Datasets (2018)](https://arxiv.org/abs/1803.09010)** â€“ *Gebru et al.* Standardized documentation for dataset provenance.
* **[Ethical Artificial Intelligence (2014)](https://arxiv.org/abs/1409.1785)** â€“ *Bill Hibbard*. Examines reward hacking and transparency in agent design.

---

## Ethics, Safety & AI Alignment
*Understanding and mitigating ethical risks, fairness concerns, and alignment challenges.*

### ğŸ›ï¸ Foundational Papers

* **[Concrete Problems in AI Safety (2016)](https://arxiv.org/abs/1606.06565)** â€“ *Amodei et al.* OpenAI paper outlining side-effects, reward hacking, oversight, safe exploration, and robustness.
* **[Ethical Artificial Intelligence (2014)](https://arxiv.org/abs/1409.1785)** â€“ *Bill Hibbard*. Free book-length treatment of utility-based agents, reward corruption, and instrumental goals.

### ğŸ“š Fairness & Bias

* **[Fairness and Machine Learning (2023)](https://fairmlbook.org/)** â€“ *Barocas, Hardt & Narayanan*. Free MIT Press book on social, legal, and technical aspects of algorithmic bias.
* **[A Tutorial on Fairness in ML (2022)](https://arxiv.org/abs/2201.01729)** â€“ *Nargesian et al.* Survey of fairness definitions and bias mitigation.
* **[Bias and Fairness in LLMs: A Survey (2024)](https://direct.mit.edu/coli/article/50/3/1097/121961/Bias-and-Fairness-in-Large-Language-Models-A)** â€“ ğŸ†• *Gallegos et al.* Computational Linguistics 82-page definitive survey.

### âš–ï¸ AI Governance & Regulation ğŸ†•

* **[EU AI Act (Regulation 2024/1689)](https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai)** â€“ ğŸ†• Entered force Aug 1, 2024. Risk-based framework: Unacceptable â†’ High â†’ Limited â†’ Minimal. [Full text](https://artificialintelligenceact.eu/)
* **[Stanford HAI AI Index (Annual)](https://aiindex.stanford.edu/)** â€“ Yearly index tracking AI development, adoption, and governance trends.
* **[AI Now Institute Reports](https://ainowinstitute.org/)** â€“ Annual reports on AI's societal impacts and policy recommendations.

### ğŸ”¬ Alignment Research from Major Labs ğŸ†•

#### Anthropic ([alignment.anthropic.com](https://alignment.anthropic.com/))
* **[Constitutional AI (2022)](https://arxiv.org/abs/2212.08073)** â€“ RLAIF, self-critique/revision methodology
* **[Alignment Faking in LLMs (Dec 2024)](https://www.anthropic.com/research/alignment-faking)** â€“ ğŸ†• **Critical**: First empirical evidence of LLMs strategically faking alignment
* **[Reasoning Models Don't Always Say What They Think (2025)](https://www.anthropic.com/research)** â€“ ğŸ†• Questions CoT monitoring for safety

#### Cross-Lab Collaboration
* **[OpenAI-Anthropic Joint Safety Evaluation (Aug 2025)](https://openai.com/index/openai-anthropic-safety-evaluation/)** â€“ ğŸ†• First-of-kind collaborative safety testing

### ğŸ“° Safety Blogs & Resources

* **[Anthropic Alignment Science Blog](https://alignment.anthropic.com/)** â€“ ğŸ†• Research on alignment and interpretability
* **[OpenAI Safety Blog](https://openai.com/safety)** â€“ Posts on concrete safety experiments
* **[DeepMind Safety Research](https://deepmind.google/discover/blog/)** â€“ Articles on scalable alignment approaches
* **[Future of Life Institute AI Safety Index](https://futureoflife.org/)** â€“ Safety metrics and assessments

### ğŸ“– Books & Courses

* **[The Alignment Problem (2020)](https://brianchristian.org/the-alignment-problem/)** â€“ *Brian Christian*. Accessible introduction to AI alignment issues.
* **[IEEE Ethically Aligned Design (2019)](https://ethicsinaction.ieee.org/)** â€“ Guidelines on accountability, privacy, and autonomous systems.
* **[Stanford "Ethics of AI" Course](https://hai.stanford.edu/)** â€“ Syllabus and readings from Stanford's AI ethics classes.
* **[ACM FAccT Conference Proceedings](https://dl.acm.org/conference/facct)** â€“ Annual proceedings on fairness, accountability, and transparency.

---

## Human-AI Interaction & Collaboration
*Design practices and studies for effective, trustworthy humanâ€“AI partnerships.*

### ğŸ“‹ Design Guidelines

* **[Guidelines for Humanâ€“AI Interaction (2019)](https://www.microsoft.com/en-us/research/publication/guidelines-for-human-ai-interaction/)** â€“ *Amershi et al.* CHI paper with 18 design guidelines. **Expanded to [HAX Toolkit](https://www.microsoft.com/en-us/haxtoolkit/ai-guidelines/).**
* **[Google PAIR Guidebook (2nd Ed.)](https://pair.withgoogle.com/)** â€“ Free guide on data visualization, recommendations, and trust-design patterns. **Updated with GenAI focus.**
* **[Human-Centered AI Framework (2020)](https://arxiv.org/abs/2001.02805)** â€“ *Ben Shneiderman*. Proposes "supertools" and "centaurs" balancing automation with human control.
* **[Microsoft AI UX Cookbook](https://github.com/microsoft/ai-ux-cookbook)** â€“ Explanation types and use cases for designers.

### ğŸ¤– Human-LLM Collaboration ğŸ†•

* **[LLM-Based Human-Agent Collaboration Systems: A Survey (2025)](https://arxiv.org/abs/2505.00753)** â€“ ğŸ†• Comprehensive survey on human-AI teaming with LLMs
* **[Fostering Effective Hybrid Human-LLM Reasoning (2024)](https://www.frontiersin.org/articles/10.3389/frai.2024.1341266)** â€“ ğŸ†• Best practices for human-AI collaboration
* **[Trust and Appropriate Reliance in AI (2025)](https://www.sciencedirect.com/science/article/pii/S0747563224003820)** â€“ ğŸ†• Research on calibrating trust in AI systems

### ğŸ“Š Research Studies

* **[Teamwork Between Humans and AI (2016)](https://ojs.aaai.org/index.php/AAAI/article/view/9808)** â€“ *Kamar*. AAAI paper on mixed-initiative systems and centaur teams.
* **[Doctors and AI: Trust in ML Diagnostics (2021)](https://dl.acm.org/doi/10.1145/3411764.3445259)** â€“ CHI study on how explanation style influences clinician trust.
* **[Effects of Chatbot Personification (2017)](https://arxiv.org/abs/1706.06162)** â€“ How personalities affect user engagement and trust.

### ğŸ› ï¸ Practical Tools

* **[Human-in-the-Loop Machine Learning (2021)](https://www.oreilly.com/library/view/human-in-the-loop-machine-learning/9781617296741/)** â€“ *Robert Munro*. Manning textbook on active learning, annotation, and continual improvement.
* **[LabelStudio](https://labelstud.io/)** â€“ Open-source tool for designing effective data-labeling workflows.
* **[LangGraph HITL Patterns](https://langchain-ai.github.io/langgraph/)** â€“ ğŸ†• Human-in-the-loop patterns for agentic systems

---

## ğŸ› ï¸ Popular Tools & Frameworks

### Deep Learning Libraries

| Tool | Description | Best For | Version |
|------|-------------|----------|---------|
| **[PyTorch](https://pytorch.org/)** | Dynamic neural network framework | Research, prototyping | 2.5.x (Dec 2024) |
| **[TensorFlow](https://www.tensorflow.org/)** | Production-ready ML platform | Deployment, scaling | 2.18 (Oct 2024) |
| **[JAX](https://github.com/google/jax)** | NumPy-compatible ML library | High-performance computing | 0.4.x |
| **[Hugging Face Transformers](https://huggingface.co/)** | Pre-trained models hub | NLP, multimodal tasks | v5.0 releasing |

### ğŸ†• LLM Inference Tools

| Tool | URL | Best For |
|------|-----|----------|
| **Ollama** | [ollama.ai](https://ollama.ai/) | Local development, one-command setup |
| **vLLM** | [GitHub](https://github.com/vllm-project/vllm) | Production serving, PagedAttention |
| **llama.cpp** | [GitHub](https://github.com/ggerganov/llama.cpp) | CPU/edge inference, GGUF format |

### ğŸ†• LLM Development Frameworks

| Tool | URL | Best For |
|------|-----|----------|
| **LangChain** | [langchain.com](https://langchain.com/) | Chains, agents, memory |
| **LlamaIndex** | [llamaindex.ai](https://llamaindex.ai/) | RAG, 150+ data connectors |
| **LangGraph** | [GitHub](https://github.com/langchain-ai/langgraph) | Multi-agent systems |

### ğŸ†• Fine-tuning Tools

| Tool | URL | Best For |
|------|-----|----------|
| **Unsloth** | [unsloth.ai](https://unsloth.ai/) | 2x faster, 70% less memory |
| **Axolotl** | [GitHub](https://github.com/OpenAccess-AI-Collective/axolotl) | Comprehensive YAML-based config |
| **HF PEFT** | [GitHub](https://github.com/huggingface/peft) | LoRA, IA3, prefix tuning |

### ğŸ†• Vector Databases

| Tool | URL | Best For |
|------|-----|----------|
| **Pinecone** | [pinecone.io](https://pinecone.io/) | Managed, enterprise |
| **Chroma** | [trychroma.com](https://trychroma.com/) | Open-source, development |
| **Weaviate** | [weaviate.io](https://weaviate.io/) | Hybrid search |
| **Qdrant** | [qdrant.tech](https://qdrant.tech/) | Performance + filtering |

### ğŸ†• Evaluation Frameworks

| Tool | URL | Best For |
|------|-----|----------|
| **DeepEval** | [GitHub](https://github.com/confident-ai/deepeval) | RAG and fine-tuning metrics |
| **RAGAS** | [GitHub](https://github.com/explodinggradients/ragas) | RAG evaluation |
| **LM Eval Harness** | [GitHub](https://github.com/EleutherAI/lm-evaluation-harness) | 60+ academic benchmarks |

### Development Environments

- **ğŸ”— [Google Colab](https://colab.research.google.com/)** - Free GPU/TPU access (TPU v6e available)
- **ğŸ“Š [Kaggle Kernels](https://www.kaggle.com/code)** - Competition-ready notebooks
- **ğŸ  [Jupyter Lab](https://jupyter.org/)** - Local development environment
- **â˜ï¸ [Gradient](https://gradient.run/)** - Cloud-based ML workspace

---

## ğŸ“š Learning Paths & Study Plans

### ğŸ“‹ Beginner's 6-Month Journey

<details>
<summary><strong>ğŸ“… Detailed Timeline</strong></summary>

**Month 1-2: Foundations**
- [ ] Complete "AI: Foundations of Computational Agents" (Chapters 1-5)
- [ ] Watch 3Blue1Brown Neural Network series
- [ ] Learn Python basics if needed

**Month 3-4: Machine Learning**
- [ ] Study "Introduction to Statistical Learning" 
- [ ] Complete Andrew Ng's ML course exercises
- [ ] Build first ML project (iris classification)

**Month 5-6: Deep Learning**
- [ ] Work through "Neural Networks and Deep Learning"
- [ ] Implement neural network from scratch
- [ ] Choose specialization area

</details>

### ğŸ¯ Skill-Based Tracks

| Track | Duration | Key Resources | Projects |
|-------|----------|---------------|----------|
| **ğŸ”¤ NLP Specialist** | 3-4 months | Transformers, BERT papers, LangChain | Chatbot, RAG system |
| **ğŸ‘ï¸ Computer Vision** | 3-4 months | CNN papers, LLaVA, OpenCV | Image classifier, multimodal app |
| **ğŸ® Reinforcement Learning** | 4-5 months | Sutton & Barto, Gymnasium, DPO | Game AI, RLHF fine-tuning |
| **ğŸ”— MLOps Engineer** | 2-3 months | vLLM, Docker, deployment guides | Model serving, monitoring |
| **ğŸ¤– AI Agents** | 3-4 months | LangGraph, agent surveys, CoALA | Autonomous agents, tool use |

---

## ğŸ’¡ Study Tips & Best Practices

### ğŸ§  Effective Learning Strategies

1. **ğŸ“– Active Reading**: Take notes and implement code examples
2. **ğŸ› ï¸ Project-Based Learning**: Build something with each new concept
3. **ğŸ‘¥ Community Engagement**: Join Discord servers and forums
4. **ğŸ“ Teaching Others**: Write blog posts or explain concepts
5. **ğŸ”„ Spaced Repetition**: Review concepts at increasing intervals

### ğŸš« Common Pitfalls to Avoid

- âŒ **Jumping to advanced topics too quickly**
- âŒ **Only reading without implementing**
- âŒ **Ignoring mathematical foundations**
- âŒ **Not practicing on real datasets**
- âŒ **Comparing your progress to experts**

---

## ğŸŒ Community & Discussion

### ğŸ’¬ Active Communities

- **ğŸ”´ [r/MachineLearning](https://www.reddit.com/r/MachineLearning/)** - Research discussions (2.8M+ members)
- **ğŸ’™ [r/LearnMachineLearning](https://www.reddit.com/r/LearnMachineLearning/)** - Beginner-friendly
- **ğŸ¦™ [r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/)** - ğŸ†• Open-source LLMs (595K+ members)
- **ğŸ¤— [Hugging Face Discord](https://discord.gg/huggingface)** - 1.3M+ models community
- **ğŸ§  [Learn AI Together Discord](https://discord.gg/learnaitogether)** - 70K+ members

### ğŸ“º YouTube Channels

- **[Andrej Karpathy](https://www.youtube.com/c/AndrejKarpathy)** - "Neural Networks: Zero to Hero" - Essential. Founded Eureka Labs (2024)
- **[3Blue1Brown](https://www.youtube.com/c/3blue1brown)** - Mathematical intuition
- **[Two Minute Papers](https://www.youtube.com/c/KÃ¡rolyZsolnai)** - Research summaries
- **[Yannic Kilcher](https://www.youtube.com/c/YannicKilcher)** - In-depth paper reviews
- **[Lex Fridman](https://www.youtube.com/c/lexfridman)** - AI leader interviews
- **[StatQuest](https://www.youtube.com/c/joshstarmer)** - ML math foundations

### ğŸ“° Newsletters ğŸ†•

| Newsletter | Frequency | Description |
|------------|-----------|-------------|
| **[The Batch](https://www.deeplearning.ai/the-batch/)** | Weekly | Andrew Ng's authoritative AI news |
| **[The Rundown AI](https://www.therundown.ai/)** | Daily | 600K+ readers |
| **[AlphaSignal](https://alphasignal.ai/)** | Weekly | Technical focus, 180K+ subscribers |
| **[TLDR AI](https://tldr.tech/ai)** | Daily | Quick research summaries |

---

## ğŸ“ University Courses (2024-2025) ğŸ†•

| Course | Institution | URL | Notes |
|--------|-------------|-----|-------|
| CS224N (NLP with Deep Learning) | Stanford | [Website](https://web.stanford.edu/class/cs224n/) | Free 2024 YouTube playlist |
| CS329A (AI Agents) | Stanford | [Catalog](https://explorecourses.stanford.edu/) | **New 2025**: Agentic workflows |
| 6.S191 (Intro to Deep Learning) | MIT | [Website](https://introtodeeplearning.com/) | Open-sourced materials |
| 6.5940 (TinyML) | MIT | [Website](https://hanlab.mit.edu/courses/2024-fall-65940) | Deploy LLMs on laptops |
| 10-423 (Generative AI) | CMU | [Website](https://www.cs.cmu.edu/~mgormley/) | **New Fall 2025** |

---

## â“ Frequently Asked Questions

<details>
<summary><strong>ğŸ¤” "I'm completely new to programming. Where should I start?"</strong></summary>

Start with Python basics first:
1. **[Python for Everybody](https://www.py4e.com/)** - Free course by Dr. Chuck
2. **[Automate the Boring Stuff](https://automatetheboringstuff.com/)** - Practical Python
3. Then move to AI foundations in this compendium

</details>

<details>
<summary><strong>ğŸ“š "How much math do I need to know?"</strong></summary>

**Essential Math Topics:**
- **Linear Algebra**: Vectors, matrices, eigenvalues
- **Calculus**: Derivatives (for backpropagation)
- **Statistics**: Probability, distributions, hypothesis testing
- **Discrete Math**: Logic, set theory (for symbolic AI)

**Great Resources:**
- **[Khan Academy](https://www.khanacademy.org/)** - All math topics
- **[3Blue1Brown Essence Series](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)** - Visual linear algebra

</details>

<details>
<summary><strong>ğŸ’¼ "Which AI career path should I choose?"</strong></summary>

**Career Paths by Interest:**
- **Love Research?** â†’ AI Researcher / PhD track
- **Want to Build Products?** â†’ ML Engineer / AI Product Manager  
- **Enjoy Data Analysis?** â†’ Data Scientist / AI Analyst
- **Like Infrastructure?** â†’ MLOps Engineer / AI Platform Engineer
- **Interested in Ethics?** â†’ AI Safety Researcher / AI Policy Specialist
- **Want to Build Agents?** â†’ AI Engineer (emerging role) ğŸ†•

</details>

<details>
<summary><strong>â±ï¸ "How long does it take to become job-ready?"</strong></summary>

**Realistic Timelines:**
- **Career Switcher (Full-time study)**: 6-12 months
- **Student (Part-time)**: 1-2 years  
- **Working Professional (Weekends)**: 1.5-3 years
- **PhD Research Track**: 4-7 years

</details>

---

## ğŸ¯ Career Roadmaps

### ğŸ› ï¸ ML Engineer Path (6-12 months)

```
Month 1-3: Foundations
â”œâ”€â”€ Python programming
â”œâ”€â”€ Statistics & linear algebra
â””â”€â”€ Basic ML algorithms

Month 4-6: Deep Learning
â”œâ”€â”€ Neural networks
â”œâ”€â”€ PyTorch/TensorFlow
â””â”€â”€ Computer vision OR NLP specialization

Month 7-9: Production Skills
â”œâ”€â”€ MLOps (Docker, Kubernetes)
â”œâ”€â”€ Model deployment (vLLM, Ollama)
â””â”€â”€ System design

Month 10-12: Portfolio & Job Search
â”œâ”€â”€ 3-5 strong projects
â”œâ”€â”€ Open source contributions
â””â”€â”€ Technical interviews prep
```

### ğŸ¤– AI Engineer Path (2024+) ğŸ†•

```
Month 1-2: LLM Foundations
â”œâ”€â”€ Transformer architecture
â”œâ”€â”€ Prompt engineering
â””â”€â”€ API usage (OpenAI, Anthropic, etc.)

Month 3-4: RAG & Agents
â”œâ”€â”€ LangChain/LlamaIndex
â”œâ”€â”€ Vector databases
â””â”€â”€ Agent architectures

Month 5-6: Production & Evaluation
â”œâ”€â”€ Fine-tuning (LoRA, PEFT)
â”œâ”€â”€ Evaluation frameworks
â””â”€â”€ Deployment patterns

Month 7+: Specialization
â”œâ”€â”€ Multimodal systems
â”œâ”€â”€ Agentic workflows
â””â”€â”€ Safety & alignment
```

### ğŸ”¬ AI Researcher Path (2-4 years)

```
Year 1: Strong Foundations
â”œâ”€â”€ Advanced mathematics
â”œâ”€â”€ Classical ML theory
â””â”€â”€ Programming proficiency

Year 2: Research Skills
â”œâ”€â”€ Paper reading & writing
â”œâ”€â”€ Research methodology
â””â”€â”€ Conference presentations

Year 3-4: Specialization
â”œâ”€â”€ Choose research area
â”œâ”€â”€ PhD or industry research
â””â”€â”€ Publication record
```

---

## ğŸ“… Events & Conferences

### ğŸŒŸ Premier AI Conferences

| Conference | Focus Area | 2025 | 2026 |
|------------|------------|------|------|
| **[NeurIPS](https://neurips.cc/)** | General ML/AI | December | TBA |
| **[ICML](https://icml.cc/)** | Machine Learning | Vancouver | Seoul |
| **[ICLR](https://iclr.cc/)** | Learning Representations | Singapore | Rio de Janeiro |
| **[AAAI](https://aaai.org/conference/)** | Artificial Intelligence | Philadelphia | TBA |
| **[ACL](https://www.aclweb.org/)** | Natural Language Processing | Vienna | TBA |
| **[CVPR](https://cvpr.thecvf.com/)** | Computer Vision | Nashville | TBA |

### ğŸª Community Events

- **[AI/ML Meetups](https://www.meetup.com/topics/artificial-intelligence/)** - Local networking
- **[Papers We Love](https://paperswelove.org/)** - Academic paper discussions
- **[Kaggle Days](https://kaggledays.com/)** - Data science competitions
- **[PyTorch Conference](https://pytorch.org/blog/)** - Framework-specific events

---

## ğŸ¤ Contributing

We welcome contributions! Please:

1. **Check existing resources** before suggesting duplicates
2. **Prioritize free resources** (open access, free PDFs, etc.)
3. **Include publication year** for papers and books
4. **Verify links are working** before submitting
5. **Add brief descriptions** explaining why the resource is valuable

---

<div align="center">

**â­ If this compendium helped you on your AI journey, please consider giving it a star! â­**

*Made with â¤ï¸ by the AI community, for the AI community*

*Last updated: January 2025*

**[ğŸ” Back to Top](#-comprehensive-artificial-intelligence-reading-compendium)**

</div>
